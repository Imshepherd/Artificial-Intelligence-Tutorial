# 梯度下降法

1. 基本知識

資料科學是一門探索未知的學問，我們將運用現有的資料，建構資料之間的關聯性。

而我們該如何開始進行預測呢？

在數學上，假定我們的Input是x物件，而Output是y物件，那我們可以建構一個『預測函數』f()來對其進行預測：

ŷ = f(x)

而在這裡f()是代表連接x與y任意型態的函數，假定我們以經典的簡單線性迴歸為例，那他將可以改寫為：

ŷ = f(x) = b0 + b1 * x1

但凡預測就會存在誤差，在這裡我們再定義一個『損失函數』diff()：

loss = diff(y, ŷ)

以簡單線性迴歸為例，此式改寫為：

loss = diff(y, ŷ) = mean((y - ŷ)^2)

由於『損失函數』內存在著『預測函數』的Output，因此我們可以簡單的將『預測函數』與『損失函數』進行合併：

loss = diff(y, f(x))

我們先以線性迴歸為例在上式進行全部展開，可以獲得下列結果

loss = diff(y, f(x)) = mean((y - (b0 + b1 * x1))^2)

顯而易見的是，我們會希望我們會希望得到一個『預測函數』使loss越小越好，因此我們可以將上述問題轉變為一個『優化問題』(求函數極值)：

minimize loss

所以，至此為止我們要了解到，其實我們的任務其實就是給定一組『預測函數』與『損失函數』，而其中『預測函數』將包含若干『待解參數』，而透過上述求解『優化問題』的過程，我們將可以獲得所有的『待解參數』，而在最後我們就能使用『預測函數』進行後續的預測。

2. 極值問題

在高中時代，我們就曾經學過如何求解函數極值，最有效率的方法是使用『微分』工具。舉例來說，我們現在希望求得下列函數的極值：

f(x) = x^2 + 2*x - 1

接著，我們對上述函數進行微分，並尋找微分後函數為0的位置，將可以知道此函數的極值位置：

f(x)' = 2x + 2 = 0
x = -1

為什麼我們能夠利用『微分』求函數的極值？這邊大家可能要複習一下基本觀念，對一個『函數』進行『微分』所獲得的『導函數』其實就是該函數的『切線斜率函數』，而『切線斜率函數』等於0的位置就暗示著函數不經過一系列的上升/下降後停止變化，那當然這個位置就是極值所在。

然而，剛剛的求極值過程中有一個非常討厭的部分，那就是要求一個「一元一次方程式」，而當函數複雜一點，我們將要求「N元M次聯立方程式」的答案，那將會讓整個過程異常複雜，所以我們要尋求其他解決方案。

在這裡我們隆重介紹『梯度下降/上升法』。首先我們要先定義何謂『梯度』？所謂的『梯度』其實就是『斜率』（注意，這個定義並不精確，但為了省略太多複雜的數學語言，我們暫且使用這個定義）。在這個定義之下，『梯度下降/上升法』意思就是我們在『求解極值』的過程中，隨著『梯度』進行移動，從而找到極值的過程。

(過程略...)

下面以找到函數「f(x) = x^2 + 2*x - 1」的極值為例，我們先隨機指定一個起始值，並定義他是第0代：

value(epoch = 0) = 10

接著，我們定義一下梯度下降法的公式(lr為學習率，一般我們會給一個很小的值，如下面的範例我們將使用0.05)：

value(epoch = t) = value(epoch = t - 1) - lr × f(value(epoch = t - 1))' 

由於剛剛函數的導函數為「f(x)' = 2x + 2」，我們可以將式子帶入運算：

value(epoch = 1) = value(epoch = 0) - lr × f(value(epoch = 0))' 
                 = 10 - 0.05 × f(10)'
                 = 10 - 0.05 × (2 × 10 + 2)
                 = 10 - 1.1
                 = 8.9

獲得第一代的值之後，我們依序可以獲得第二代的值

value(epoch = 2) = value(epoch = 1) - lr × f(value(epoch = 1))' 
                 = 8.9 - 0.05 × f(8.9)'
                 = 8.9 - 0.05 × (2 × 8.9 + 2)
                 = 8.9 - 0.99
                 = 7.91
                 
持續進行...

value(epoch = 3) = 7.91   - 0.891   = 7.019
value(epoch = 4) = 7.019  - 0.8019  = 6.2171
value(epoch = 5) = 6.2171 - 0.72171 = 5.49539

...

value(epoch = ∞) = -1

因此，我們現在學到一個技巧用來解函數極值，再讓我們回到『預測函數』吧！

3. 偏微分(向量/矩陣微分)








              


